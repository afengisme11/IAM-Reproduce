import os
from environments.vectorized_environment import VectorizedEnvironment
import argparse
import yaml
import time
import sacred
from sacred.observers import MongoObserver

import copy
import glob
import os
from collections import deque

import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from a2c_ppo_acktr import algo, utils
from a2c_ppo_acktr.model import Policy
from a2c_ppo_acktr.storage import RolloutStorage

class Experimentor(object):
    """
    Creates experimentor object to store interact with the environment and
    the agent and log results.
    """

    def __init__(self, parameters, _run, seed):
        """
        Initializes the experiment by extracting the parameters
        @param parameters a dictionary with many obligatory elements
        <ul>
        <li> "env_type" (SUMO, atari, grid_world),
        <li> algorithm (DQN, PPO)
        <li> maximum_time_steps
        <li> maximum_episode_time
        <li> skip_frames
        <li> save_frequency
        <li> step
        <li> episodes
        and more TODO
        </ul>

        @param logger  TODO what is it exactly? It must have the function
        log_scalar(key, stat_mean, self.step[factor_i])
        """
        self.parameters = parameters
        self.path = self.generate_path(self.parameters)
        self.generate_env(seed)
        self.train_frequency = self.parameters["train_frequency"]

    def generate_path(self, parameters):
        """
        Generate a path to store e.g. logs, models and plots. Check if
        all needed subpaths exist, and if not, create them.
        """
        path = self.parameters['name']
        result_path = os.path.join("results", path)
        model_path = os.path.join("models", path)
        if not os.path.exists(result_path):
            os.makedirs(result_path)
        if not os.path.exists(model_path):
            os.makedirs(model_path)
        return path

    def generate_env(self, seed):
        """
        Create environment container that will interact with SUMO
        """
        self.env = VectorizedEnvironment(self.parameters, seed)

    # def print_results(self, info, n_steps=0):
    #     """
    #     Prints results to the screen.
    #     """
    #     if self.parameters['env_type'] == 'atari':
    #         print(("Train step {} of {}".format(self.step,
    #                                             self.maximum_time_steps)))
    #         print(("-"*30))
    #         print(("Episode {} ended after {} steps.".format(
    #                                     self.controller.episodes,
    #                                     info['l'])))
    #         print(("- Total reward: {}".format(info['r'])))
    #         print(("-"*30))
    #     else:
    #         print(("Train step {} of {}".format(self.step,
    #                                             self.maximum_time_steps)))
    #         print(("-"*30))
    #         print(("Episode {} ended after {} steps.".format(
    #                                     self.controller.episodes,
    #                                     n_steps)))
    #         print(("- Total reward: {}".format(info)))
    #         print(("-"*30))


    def run(self):
        """
        Runs the experiment.
        """
        args = dict(
            alpha=0.99, clip_param=0.2, entropy_coef=0.01, eps=1e-05, 
            eval_interval=None, gamma=0.99, log_dir='/tmp/gym/', 
            log_interval=10, lr=0.0007, max_grad_norm=0.5, num_env_steps=10000000.0, 
            num_mini_batch=32, num_processes=16, num_steps=5, recurrent_policy=False, 
            save_dir='./trained_models/', save_interval=100, seed=1, use_linear_lr_decay=False, 
            use_proper_time_limits=False, value_loss_coef=0.5
        )

        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.maximum_time_steps = int(self.parameters["max_steps"])
        self.step = max(self.parameters["iteration"], 0)

        actor_critic = Policy(
            envs.observation_space.shape,
            envs.action_space,
            base_kwargs={'recurrent': args.recurrent_policy})
        actor_critic.to(device)

        agent = algo.A2C_ACKTR(
            actor_critic,
            args['value_loss_coef'],
            args['entropy_coef'],
            lr=args['lr'],
            eps=args['eps'],
            alpha=args['alpha'],
            max_grad_norm=args['max_grad_norm'])

        rollouts = RolloutStorage(args.num_steps, args.num_processes,
                                envs.observation_space.shape, envs.action_space,
                                actor_critic.recurrent_hidden_state_size)
        # Reset environment
        step_output = self.env.reset()
        reward = 0
        n_steps = 0


        


        self.env.close()

def get_parameters():
    parser = argparse.ArgumentParser(description='RL')
    parser.add_argument('--config', default=None, help='config file')
    parser.add_argument('--scene', default=None, help='scene')
    parser.add_argument('--flicker', action='store_true', help='flickering game')
    args = parser.parse_args()
    return args

def read_parameters(config_file):
    with open(config_file) as file:
        parameters = yaml.load(file, Loader=yaml.FullLoader)
    return parameters['parameters']


def add_mongodb_observer():
    """
    connects the experiment instance to the mongodb database
    """
    print("ONLY FILE STORAGE OBSERVER ADDED")
    from sacred.observers import FileStorageObserver
    ex.observers.append(FileStorageObserver.create('saved_runs'))

ex = sacred.Experiment('influence-aware-memory')
ex.add_config('configs/default.yaml')
add_mongodb_observer()

@ex.automain
def main(parameters, seed, _run):
    exp = Experimentor(parameters, _run, seed)
    exp.run()
